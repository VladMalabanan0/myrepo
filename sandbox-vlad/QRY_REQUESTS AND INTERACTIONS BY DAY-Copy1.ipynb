{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import mmh3\n",
    "import os\n",
    "import shutil\n",
    "import string\n",
    "import textwrap\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import dask\n",
    "import dask.multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Make sure you `pip install bt-ai[dev]` for these (or you can use the [prod] dependencies if you like)\n",
    "from bt_ai.stable.data_input.dataframe import MultiDataFrameLoader, DataFrameTarget\n",
    "from bt_ai.stable.data_input.redshift import UnloadQuery, UnloadTask, HourlyEventDumpQuery, RawEventDumpQuery, RawEventQueryWithSession\n",
    "from bt_ai.stable.data_input.resources import ResourcesDb, ResourceDump\n",
    "\n",
    "# Make sure you `pip install bt-notebook-utils` for these\n",
    "from notebook_utils.logging import setup_logging\n",
    "from notebook_utils.luigi import run_luigi_tasks\n",
    "from notebook_utils.s3 import delete_s3_folder\n",
    "from notebook_utils.sequences import daterange, hourrange, date_compressed_hourrange, pairwise\n",
    "\n",
    "\n",
    "# Use this to get multi-processing with out-of-core processing\n",
    "dask.set_options(get=dask.multiprocessing.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDSHIFT_CREDENTIALS = json.loads(os.environ.get('REDSHIFT_CREDENTIALS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section sets up the logging, so running Luigi jobs produces output in this notebook\n",
    "\n",
    "# Check `notebook_utils.logging.LOGGER_OVERRIDES` for the default logger overrides, \n",
    "# or, optionally, pass in your own additional overrides (which can override LOGGER_OVERRIDES)\n",
    "logging_overrides = {\n",
    "    'luigi-interface': logging.INFO,\n",
    "}\n",
    "setup_logging(level=logging.DEBUG, overrides=logging_overrides)\n",
    "# Use any name you want for this logger.  \n",
    "LOG = logging.getLogger('jupyter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the site_id and start/end dates to be something sane\n",
    "#site_id = 'wnyt-hubbard-tv'\n",
    "today=datetime.today()\n",
    "_7ago=datetime.today() - timedelta(days=7)\n",
    "\n",
    "end_date=datetime(today.year,today.month, today.day)\n",
    "start_date = datetime(_7ago.year,_7ago.month, _7ago.day)\n",
    "\n",
    "# make sure you change this to your own sandbox bucket on S3\n",
    "s3_data_bucket = 'vladm-sandbox'\n",
    "s3_data_path = 's3://' + s3_data_bucket\n",
    "local_data_path = 'output_data'\n",
    "luigi_planner_uri = 'http://localhost:8082'\n",
    "\n",
    "model_store_path = local_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from requests and interactions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data to from recs.requests table\n",
    "dump_sub_folder = 'requests-interactions'\n",
    "\n",
    "#delete_s3_folder(s3_data_bucket, dump_sub_folder)\n",
    "\n",
    "query_template = textwrap.dedent('''\\\n",
    "                WITH \n",
    "                    a AS (\n",
    "                        SELECT \n",
    "                          date(event_time) AS request_day, \n",
    "                          site_id, \n",
    "                          medium, \n",
    "                          recset \n",
    "                        FROM \n",
    "                          recs.requests \n",
    "                        WHERE \n",
    "                          event_time < '{end_date}' \n",
    "                          AND event_time >= '{start_date}')\n",
    "                    ,b AS (\n",
    "                        SELECT DISTINCT \n",
    "                          site_id,\n",
    "                          recset,\n",
    "                          event_type,\n",
    "                          min(date(event_time)) AS action_day\n",
    "                        FROM \n",
    "                          recs.interactions\n",
    "                        WHERE \n",
    "                          event_time < '{end_date}' \n",
    "                          AND event_time >= '{start_date}'\n",
    "                        GROUP BY \n",
    "                          site_id, recset, event_type)\n",
    "                SELECT \n",
    "                  a.request_day,\n",
    "                  a.site_id,\n",
    "                  b.event_type, \n",
    "                  b.action_day, \n",
    "                  a.medium, \n",
    "                  count(DISTINCT a.recset) as count\n",
    "                FROM \n",
    "                  a \n",
    "                LEFT JOIN \n",
    "                  b\n",
    "                ON\n",
    "                  a.recset = b.recset\n",
    "                GROUP BY\n",
    "                  event_type, request_day, action_day, a.site_id, a.medium\n",
    "                ORDER BY \n",
    "                  event_type, request_day, action_day, a.site_id, a.medium\n",
    "            ''')\n",
    "\n",
    "s3_unload_path_template = '''s3://{root}/requests-interactions/s{start_date}.e{end_date}'''\n",
    "\n",
    "DATETIME_FORMAT = '%Y%m%dT%H%M%S'\n",
    "    \n",
    "daily_requests = [\n",
    "    UnloadTask(\n",
    "        redshift_query=UnloadQuery(\n",
    "            query=query_template.format(\n",
    "                #site_id=site_id,\n",
    "                start_date=s,\n",
    "                end_date=e\n",
    "            ),\n",
    "            column_names=['request_day', 'site_id','event_type','action_day', 'medium', 'count'],\n",
    "            s3_unload_path=s3_unload_path_template.format(\n",
    "                root=s3_data_bucket,\n",
    "#                 site_id=site_id,\n",
    "#                 start_date=start_date.strftime(DATETIME_FORMAT),\n",
    "#                 end_date=end_date.strftime(DATETIME_FORMAT)\n",
    "                start_date=s.strftime(DATETIME_FORMAT),\n",
    "                end_date=e.strftime(DATETIME_FORMAT)\n",
    "            ),\n",
    "            index_columns=['request_day', 'site_id','action_day'],\n",
    "            date_columns=['request_day','action_day']\n",
    "        ),\n",
    "        redshift_credentials=REDSHIFT_CREDENTIALS\n",
    "    )\n",
    "    for s, e in pairwise(daterange(start_date, end_date))\n",
    "]\n",
    "\n",
    "run_luigi_tasks(daily_requests, scheduler_uri=luigi_planner_uri, multiprocess=True, num_processes=8)\n",
    "\n",
    "LOG.info('Loading dumped data')\n",
    "requests_loader = MultiDataFrameLoader.create_multi_dataframe_target(\n",
    "    [task.output() for task in daily_requests], \n",
    "    compute=False\n",
    ")\n",
    "\n",
    "with requests_loader.open('r') as infile:\n",
    "    requests_ddf = infile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from interactions table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Extract data to from recs.interactions table\n",
    "dump_sub_folder = 'interactions'\n",
    "\n",
    "#delete_s3_folder(s3_data_bucket, dump_sub_folder)\n",
    "\n",
    "query_template = textwrap.dedent('''\\\n",
    "                SELECT DISTINCT \n",
    "                  site_id,\n",
    "                  recset,\n",
    "                  event_type,\n",
    "                  min(date(event_time)) AS action_day\n",
    "                FROM recs.interactions\n",
    "                WHERE event_time < '{end_date}' \n",
    "                  AND event_time >= '{start_date}' \n",
    "                GROUP BY site_id,\n",
    "                  recset,\n",
    "                  event_type\n",
    "            ''')\n",
    "\n",
    "s3_unload_path_template = '''s3://{root}/interactions/s{start_date}.e{end_date}'''\n",
    "\n",
    "DATETIME_FORMAT = '%Y%m%dT%H%M%S'\n",
    "    \n",
    "daily_interactions = [\n",
    "    UnloadTask(\n",
    "        redshift_query=UnloadQuery(\n",
    "            query=query_template.format(\n",
    "                start_date=s,\n",
    "                end_date=e\n",
    "            ),\n",
    "            column_names=['site_id', 'recset', 'event_type', 'action_day'],\n",
    "            s3_unload_path=s3_unload_path_template.format(\n",
    "                root=s3_data_bucket,\n",
    "#                 site_id=site_id,\n",
    "#                 start_date=start_date.strftime(DATETIME_FORMAT),\n",
    "#                 end_date=end_date.strftime(DATETIME_FORMAT)\n",
    "                start_date=s.strftime(DATETIME_FORMAT),\n",
    "                end_date=e.strftime(DATETIME_FORMAT)\n",
    "            ),\n",
    "            index_columns=['recset', 'site_id'],\n",
    "            date_columns=['action_day']\n",
    "        ),\n",
    "        redshift_credentials=REDSHIFT_CREDENTIALS\n",
    "    )\n",
    "    for s, e in pairwise(daterange(start_date, end_date))\n",
    "]\n",
    "\n",
    "run_luigi_tasks(daily_interactions, scheduler_uri=luigi_planner_uri, multiprocess=True, num_processes=8)\n",
    "\n",
    "LOG.info('Loading dumped data')\n",
    "interactions_loader = MultiDataFrameLoader.create_multi_dataframe_target(\n",
    "    [task.output() for task in daily_interactions], \n",
    "    compute=False\n",
    ")\n",
    "\n",
    "with interactions_loader.open('r') as infile:\n",
    "    interactions_ddf = infile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge requests and interactions table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "merge_ddf=dask.dataframe.merge(requests_ddf, interactions_ddf, on=['recset', 'site_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "merge_ddf=merge_ddf.fillna(\"No Action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate to count requests by day by event type and by event day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aggregate_ddf=merge_ddf.groupby(['site_id', 'request_day', 'action_day', 'medium', 'event_type']).recset.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df = pd.DataFrame(dask.compute(aggregate_ddf)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
